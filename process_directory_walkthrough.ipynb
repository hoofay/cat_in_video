{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688301f5",
   "metadata": {},
   "source": [
    "## Process directory videos\n",
    "\n",
    "This notebook provides a run through of how to set up your environment, and a run through of the object detection algorithms that process video files in a chosen directory. \n",
    "\n",
    "#### Purpose\n",
    "\n",
    "The purpose of the notebook is, for a given directory, to determine which videos contain a given object (e.g. 'cat') and to save those videos to a new location. All videos will be deleted from the source directory. The idea here is that if we have a camera with a sensor that is regularly storing files into a given location, we can run this notebook to keep any files that contain a 'cat' (or other object to detect), store them somewhere safe (e.g. googledrive) and purge all other videos to keep storage costs down. \n",
    "\n",
    "#### Setup\n",
    "\n",
    "I have created a new environment using anaconda (python v3.8) and executed the following commands:\n",
    "\n",
    "``` [python]\n",
    "pip install tensorflow\n",
    "pip install opencv-python\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "The base python installation plus the libraries above should deal with all required libraries to run this notebook. Ensure that in your directory with this notebook you have the yolo-v5 model object `yolo-v5.tflite`. \n",
    "\n",
    "#### Load project libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd197f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368ee9f",
   "metadata": {},
   "source": [
    "#### Define functions\n",
    "\n",
    "Here we're going to define a number of useful functions to execute the video review process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015cb7a",
   "metadata": {},
   "source": [
    "###### Load model\n",
    "\n",
    "The purpose of this function is to load our Yolo-v5 model and extract key input and output details that we'll use to get our inputs/outputs into the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27523d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "def load_model(model_path):\n",
    "\n",
    "    # Load the TensorFlow Lite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    return input_details, output_details, interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471499bc",
   "metadata": {},
   "source": [
    "##### Get videos\n",
    "\n",
    "This function provides a list of paths to all files in a given directory with the extension .mp4 (i.e. video files!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42b6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get videos from a directory\n",
    "def get_videos(directory):\n",
    "    directory = 'test_videos_to_process'\n",
    "    video_files = glob.glob(os.path.join(directory, '*.mp4'))\n",
    "    return video_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c43f0",
   "metadata": {},
   "source": [
    "##### Set output names\n",
    "\n",
    "Here we want to make sure that each time we run the process we timestamp our outputs. So we can define our output directory, and our input file and this function will create a nice timestamped output path that we can use for our processed videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95010d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some names\n",
    "def set_output_name(video_to_process,output_directory):\n",
    "    name_prefix = Path(video_to_process).stem\n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_video_file = output_directory + '//' + name_prefix + \"_out_\" + str(timestr) + \".mp4\"\n",
    "    \n",
    "    return output_video_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fde7f",
   "metadata": {},
   "source": [
    "##### open video streams\n",
    "\n",
    "The purpose of this function is fairly self explanatory, we want to open the cv2 read and write connections for the video we're processing and the new one we're going to write. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbe05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open connections to cv2\n",
    "def open_video_streams(video_to_process,output_file_name):\n",
    "    # Open the video stream\n",
    "    video_capture = cv2.VideoCapture(video_to_process)\n",
    "    \n",
    "    # Define the output video file name and settings\n",
    "    output_frame_size = (int(video_capture.get(3)), int(video_capture.get(4)))  # Use the same size as the input video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_file_name, fourcc, 30.0, output_frame_size)\n",
    "\n",
    "    print('video connections opened')\n",
    "    return video_capture, output_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e734f",
   "metadata": {},
   "source": [
    "##### Class filter and yolo detect\n",
    "\n",
    "The two functions below took some figuring out! Essentially we want to ensure we correctly interpret the prediction outputs from Yolo-v5, by processing the outputs to something that we can read / analyse further. Each function is annotated further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1624b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model output processing functions\n",
    "\n",
    "# class filter\n",
    "def class_filter(classdata):\n",
    "    classes = []  # create a list\n",
    "    for i in range(classdata.shape[0]):         # loop through all predictions\n",
    "        classes.append(classdata[i].argmax())   # get the best classification location\n",
    "    return classes  # return classes (int)\n",
    "\n",
    "# yolo detect\n",
    "def yolo_detect(output_data):  # input = interpreter, output is boxes(xyxy), classes, scores\n",
    "    output_data = output_data[0]                \n",
    "    boxes = np.squeeze(output_data[..., :4])    \n",
    "    scores = np.squeeze( output_data[..., 4:5]) \n",
    "    classes = class_filter(output_data[..., 5:]) \n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    x, y, w, h = boxes[..., 0], boxes[..., 1], boxes[..., 2], boxes[..., 3] # xywh\n",
    "    xyxy = [x - w / 2, y - h / 2, x + w / 2, y + h / 2]  # xywh to xyxy\n",
    "\n",
    "    return xyxy, classes, scores  # output is boxes(x,y,x,y), classes(int), scores(float) [predictions length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dcd1dd",
   "metadata": {},
   "source": [
    "##### Process video with ML\n",
    "\n",
    "The function below is the crux of this notebook. This is where we parse a video frame by frame to determine whether or not our chosen objects are present, storing outputs along the way.\n",
    "\n",
    "It performs 6 main actions, looping over each frame in a video.  \n",
    "\n",
    "1. process a videos input frame into the right format for the model\n",
    "2. runs the frame through the model\n",
    "3. saves the model scores to a results list\n",
    "4. draws a rectaingle around our object on the new 'processed video'\n",
    "5. checks how many frames have been processed (if we want to break the loop)\n",
    "6. returns results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6846d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define running the model and outputs\n",
    "def process_video_with_ml(video_capture, interpreter, input_details, output_details, output_video, max_frame_break=50):\n",
    "\n",
    "    # initialise an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    # initialise a frame counter\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Store the original frame dimensions for later conversion\n",
    "        original_frame_height, original_frame_width, _ = frame.shape\n",
    "\n",
    "        # Preprocess the frame (resize, normalize, etc.) to match the model's input shape\n",
    "        resized_frame = cv2.resize(frame,(320,320))\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "        preprocessed_frame = np.expand_dims(normalized_frame, axis=0).astype(np.float32)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n",
    "        interpreter.invoke()\n",
    "        detections = interpreter.get_tensor(output_details[0]['index'])\n",
    "        xyxy, classes, scores = yolo_detect(detections) #boxes(x,y,x,y), classes(int), scores(float)\n",
    "\n",
    "        # draw on video and write results \n",
    "        for i in range(len(scores)):\n",
    "            if ((scores[i] > 0.3) and (scores[i] <= 1.0)):\n",
    "                H = frame.shape[0]\n",
    "                W = frame.shape[1]\n",
    "                xmin = int(max(1,(xyxy[0][i] * W)))\n",
    "                ymin = int(max(1,(xyxy[1][i] * H)))\n",
    "                xmax = int(min(H,(xyxy[2][i] * W)))\n",
    "                ymax = int(min(W,(xyxy[3][i] * H)))\n",
    "\n",
    "                cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"{classes[i]}: {scores[i]:.2f}\", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                results.append({\"Frame\":frame_count,\"Class\": classes[i], \"Confidence\": scores[i]})\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        output_video.write(frame)\n",
    "\n",
    "        # Increment the frame counter\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count % 10 == 0:\n",
    "            print(frame_count)\n",
    "\n",
    "        # Check if the maximum frame count has been reached\n",
    "        if frame_count >= max_frame_break:\n",
    "            break\n",
    "        \n",
    "    print('video processing complete')\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce22cb1",
   "metadata": {},
   "source": [
    "##### Close video connections\n",
    "\n",
    "This one is self explanatory. We want to release cv2 connection to the videos we're processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28f4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_video_connections(video_capture,output_video):\n",
    "    # close video connections\n",
    "    video_capture.release()\n",
    "    output_video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print('connections closed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f738b",
   "metadata": {},
   "source": [
    "##### Is in video? \n",
    "\n",
    "Here we're running a comparison between the model outputs dataframe and our class threshold list to check whether the object is in a video file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7bbfb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_video(class_id,threshold,results):\n",
    "    checks = []\n",
    "    for i in results.index:\n",
    "        if results.iloc[i,1] == class_id and results.iloc[i,2] >= threshold:\n",
    "            checks.append(True)\n",
    "        else:\n",
    "            checks.append(False)\n",
    "    if True in checks:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a95a2",
   "metadata": {},
   "source": [
    "##### Keep video\n",
    "\n",
    "This is a logic that combines the above function with a decision process to output true/false. The idea here is that we can build this logic into a process to decide to delete videos (or not). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea213ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_video(criteria,results):\n",
    "    decision = False\n",
    "    for i in criteria.index:\n",
    "        print('checking ' + str(criteria.iloc[i,1]))\n",
    "        if is_in_video(criteria.iloc[i,0],criteria.iloc[i,2],results):\n",
    "            decision = True\n",
    "            print(True)\n",
    "        else:\n",
    "            print(False)\n",
    "    print('outcome: ' + str(decision))\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4bc7e2",
   "metadata": {},
   "source": [
    "##### Delete file\n",
    "\n",
    "We need a process to delete a file from a path. Here it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3893fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file(path):\n",
    "    os.remove(path)\n",
    "    print(path + ' file deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb24bd",
   "metadata": {},
   "source": [
    "##### Move a video\n",
    "\n",
    "We also need a process to move a file from one path to another (i.e. out of the source location that we're purging and into our nice safe 'keep' folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ce52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relocate a successful file process to output folder\n",
    "def move_video(vid,new_dir):\n",
    "    new_loc = new_dir + '//' + Path(vid).stem + '.mp4'\n",
    "    Path(vid).rename(new_loc)\n",
    "    print(vid + ' file moved to ' + new_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de516a2",
   "metadata": {},
   "source": [
    "#### Execute the process\n",
    "\n",
    "Here we're going to execute the functions, which broadly falls into the following steps:\n",
    "\n",
    "1. set parameters for execution\n",
    "2. load our model and get our input videos\n",
    "3. iterate over each video\n",
    "    a. determine whether our chosen objects (cats and teddy bears) are in the videos\n",
    "    b. if so, move processed videos to output directory\n",
    "    c. if not, delete videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a432467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set my params\n",
    "chosen_model = \"yolo-v5.tflite\"\n",
    "chosen_source_directory = 'test_videos_to_process'\n",
    "chosen_output_directory = 'processed_videos'\n",
    "chosen_frame_breaks = 10 # chosen cut off frame number for processing videos\n",
    "d = {'class': [15, 77],'class_name': ['cat','teddy bear'] , 'threshold': [0.5, 0.5]} # see list here: https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml\n",
    "my_criteria = pd.DataFrame(data=d) # set my parameters for keeping videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71964d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video connections opened\n",
      "10\n",
      "video processing complete\n",
      "connections closed\n",
      "checking cat\n",
      "False\n",
      "checking teddy bear\n",
      "False\n",
      "outcome: False\n",
      "processed_videos//apples_out_20231006_163445.mp4 file deleted\n",
      "test_videos_to_process\\apples.mp4 file deleted\n",
      "video connections opened\n",
      "10\n",
      "video processing complete\n",
      "connections closed\n",
      "checking cat\n",
      "True\n",
      "checking teddy bear\n",
      "False\n",
      "outcome: True\n",
      "test_videos_to_process\\cat.mp4 file moved to processed_videos//cat.mp4\n",
      "process complete\n"
     ]
    }
   ],
   "source": [
    "# run process\n",
    "\n",
    "input_details, output_details, interpreter = load_model(model_path=chosen_model)\n",
    "video_files = get_videos(chosen_source_directory)\n",
    "\n",
    "# run process for each video\n",
    "for i in video_files:\n",
    "    video_to_process = i\n",
    "    output_video_file = set_output_name(video_to_process,chosen_output_directory)\n",
    "    video_capture, output_video = open_video_streams(video_to_process,output_video_file)\n",
    "    results_df = process_video_with_ml(video_capture, interpreter, input_details, output_details, output_video, max_frame_break=chosen_frame_breaks)\n",
    "    close_video_connections(video_capture,output_video)\n",
    "\n",
    "    kv = keep_video(my_criteria,results_df)\n",
    "\n",
    "    if not kv:\n",
    "        delete_file(output_video_file)\n",
    "        delete_file(video_to_process)\n",
    "    else:\n",
    "        move_video(video_to_process,chosen_output_directory)\n",
    "\n",
    "print('process complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf5e7e",
   "metadata": {},
   "source": [
    "#### fin!\n",
    "\n",
    "We can see that the video containing a cat. cat.mp4 was processed (you can go and have a look at the processed video in the output folder), and the apple.mp4 file (which doesn't have a cat or a teddy bear in it!) was deleted. Success!\n",
    "\n",
    "We're done. Thanks for reading. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
